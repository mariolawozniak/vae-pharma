{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "from rdkit.Chem import BondType\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249455 entries, 0 to 249454\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   smiles  249455 non-null  object \n",
      " 1   logP    249455 non-null  float64\n",
      " 2   qed     249455 non-null  float64\n",
      " 3   SAS     249455 non-null  float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "csv_path = keras.utils.get_file(\n",
    "    \"250k_rndm_zinc_drugs_clean_3.csv\",\n",
    "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"~/.keras/datasets/250k_rndm_zinc_drugs_clean_3.csv\")\n",
    "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILE_CHARSET = '[None, \"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
    "\n",
    "bond_mapping = {\"SELF\": 0, \"SINGLE\": 1, \"DOUBLE\": 2, \"TRIPLE\": 3, \"AROMATIC\": 4}\n",
    "bond_mapping.update(\n",
    "    {0: None, 1: BondType.SINGLE, 2: BondType.DOUBLE, 3: BondType.TRIPLE, 4: BondType.AROMATIC}\n",
    ")\n",
    "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
    "MAX_BONDNUM = 150\n",
    "MAX_MOLSIZE = 64 #max(df[\"smiles\"].str.len())\n",
    "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
    "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
    "atom_mapping = dict(SMILE_to_index)\n",
    "atom_mapping.update(index_to_SMILE)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 500\n",
    "\n",
    "VAE_LR = 5e-4\n",
    "NUM_ATOMS = 32  # Maximum number of atoms\n",
    "\n",
    "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
    "BOND_DIM = 5   # Number of bond types\n",
    "LATENT_DIM = 128  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data, test_data = model_selection.train_test_split(df, test_size=0.2)\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_to_graph(molecule):\n",
    "    # Converts SMILES to molecule object\n",
    "    \n",
    "    mol_atoms = molecule.GetNumAtoms()    # Initialize adjacency and feature tensor\n",
    "    num_of_bonds = molecule.GetNumBonds()\n",
    "    edge_index = [[],[]]\n",
    "    edge_features = []\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM))\n",
    "\n",
    "    # loop over each atom in molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        atom_idx = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        atomic_num = atom.GetAtomicNum()\n",
    "        degree = atom.GetDegree()\n",
    "        #mass = atom.GetMass()\n",
    "        edge_index[0].append(atom_idx)\n",
    "        edge_index[1].append(atom_idx)\n",
    "        edge_embbeding = list(np.eye(BOND_DIM)[0])\n",
    "        edge_features.append(edge_embbeding)\n",
    "        #chem_features = [atomic_num, degree] #, mass]\n",
    "        #print(atom_idx)\n",
    "        features[atom_idx] = np.eye(ATOM_DIM)[atom_type] #np.concatenate((np.eye(ATOM_DIM)[atom_type][:-2], chem_features), axis=0)\n",
    "    for i in range(mol_atoms, NUM_ATOMS):\n",
    "        features[i][0] = 1\n",
    "        edge_index[0].append(i)\n",
    "        edge_index[1].append(i)\n",
    "        edge_embbeding = list(np.eye(BOND_DIM)[0])\n",
    "        edge_features.append(edge_embbeding)\n",
    "        # loop over one-hop neighbors\n",
    "    for bond in molecule.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "        edge_index[0] = list(np.append(edge_index[0], [i, j]))\n",
    "        edge_index[1] = list(np.append(edge_index[1], [j, i]))\n",
    "        edge_embbeding = list(np.eye(BOND_DIM)[bond_type_idx])\n",
    "        edge_features.append(edge_embbeding)\n",
    "        edge_features.append(edge_embbeding)\n",
    "\n",
    "    \n",
    "    return tensor(features), tensor(edge_index), tensor(edge_features)\n",
    "\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    # Unpack graph\n",
    "    adjacency, features = graph\n",
    "\n",
    "    # RWMol is a molecule object intended to be edited\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Remove \"no atoms\" & atoms with no bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    # Add atoms to molecule\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    # Add bonds between atoms in molecule; based on the upper triangles\n",
    "    # of the [symmetric] adjacency tensor\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule; for more information on sanitization, see\n",
    "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    # Let's be strict. If sanitization fails, return None\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        return None\n",
    "\n",
    "    return molecule\n",
    "\n",
    "def adj_to_index(adj_matrix):\n",
    "    edge_index=[[], []]\n",
    "    for i, row in enumerate(adj_matrix):\n",
    "        adj_matrix[i][i]=1 \n",
    "        for j in range(len(row)):\n",
    "            if row[j] == 1:\n",
    "                edge_index[0].append(i)\n",
    "                #edge_index[0].append(j)\n",
    "                #edge_index[1].append(i)\n",
    "                edge_index[1].append(j)\n",
    "    return torch.tensor(edge_index)\n",
    "\n",
    "def edge_idx_to_adj(edge_index):\n",
    "    adj = np.zeros((NUM_ATOMS, NUM_ATOMS))\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        k = edge_index[0][i]\n",
    "        j = edge_index[1][i]\n",
    "        adj[k][j] = 1\n",
    "    return torch.tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __call__(self, df):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            smiles = row[\"smiles\"]\n",
    "            molecule = Chem.MolFromSmiles(smiles)\n",
    "            if molecule.GetNumAtoms() < 32:\n",
    "                features, edge_index, edge_attr = molecule_to_graph(molecule)\n",
    "            #print(features, edge_index, edge_attr)  \n",
    "                graphs.append((features, edge_index, edge_attr))\n",
    "                labels.append(molecule)\n",
    "            \n",
    "        return [Data(\n",
    "            features=features.float(), \n",
    "            edge_index=edge_index.int(), \n",
    "            edge_attr=edge_attr.float(),\n",
    "            mol=labels)\n",
    "            for ((features, edge_index, edge_attr), labels) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyDataLoader()\n",
    "data = data(df[:16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i in loader:\n",
    "    print(i.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import dropout\n",
    "import torch\n",
    "\n",
    "HIDDEN_SIZE=512\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE, atom_dim = ATOM_DIM, latent_dim=LATENT_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.conv = [\n",
    "            GATConv(atom_dim, hidden_size),\n",
    "            GATConv(hidden_size, hidden_size),\n",
    "            GATConv(hidden_size, hidden_size),\n",
    "            GATConv(hidden_size, hidden_size),\n",
    "            GATConv(hidden_size, hidden_size),\n",
    "            GATConv(hidden_size, hidden_size)\n",
    "        ]\n",
    "\n",
    "        self.dense = [\n",
    "            torch.nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        ]\n",
    "        self.mean = torch.nn.Linear(in_features=hidden_size, out_features=latent_dim)\n",
    "        self.std = torch.nn.Linear(in_features=hidden_size, out_features=latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        for conv_layer in self.conv:\n",
    "            x = conv_layer(x, edge_index, edge_attr)\n",
    "            x = torch.nn.ELU()(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch=batch, size = x.shape[0])[0]\n",
    "        #print(nodes.shape, nodes)\n",
    "        x = torch.flatten(x)\n",
    "        for layer in self.dense:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = torch.nn.ELU()(x)\n",
    "\n",
    "        mean, std = self.mean(x, edge_index, edge_attr), self.std(x, edge_index, edge_attr)\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "    def loss():\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE, atom_dim = ATOM_DIM, latent_dim=LATENT_DIM) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.edges_transform = torch.nn.Linear(in_features=latent_dim, out_features=int(NUM_ATOMS)*int(NUM_ATOMS))\n",
    "\n",
    "        self.nodes_linear1 = torch.nn.Linear(in_features=latent_dim, out_features=NUM_ATOMS)\n",
    "        self.nodes_linear2 = torch.nn.Linear(in_features=NUM_ATOMS, out_features=int(NUM_ATOMS)*int(ATOM_DIM))\n",
    "\n",
    "    def forward(self, nodes_encoded, edges_encoded, batch):\n",
    "        edges = self.edges_transform(edges_encoded)\n",
    "        #edges = torch.sigmoid(edges)\n",
    "\n",
    "        edges = torch.reshape(edges, (NUM_ATOMS, NUM_ATOMS)) + torch.transpose_copy(torch.reshape(edges, (NUM_ATOMS, NUM_ATOMS)), 0, 1)\n",
    "        #adjacency = torch.matmul(torch.unsqueeze(edges, 1), torch.unsqueeze(edges, 0))\n",
    "        #edges = torch.softmax(edges, 0, float)\n",
    "\n",
    "        nodes = self.nodes_linear1(nodes_encoded)\n",
    "        nodes = self.nodes_linear2(nodes)\n",
    "        nodes = torch.reshape(nodes, (NUM_ATOMS, ATOM_DIM))\n",
    "        #nodes = F.one_hot(torch.argmax(nodes, dim=1), ATOM_DIM)\n",
    "        return torch.tensor(nodes, dtype=float), edges\n",
    "\n",
    "    def loss():\n",
    "        pass\n",
    "\n",
    " \n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, encoder, decoder, hidden_size, atom_dim = ATOM_DIM, latent_dim=LATENT_DIM):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.encoder = Encoder(hidden_size)\n",
    "        self.decoder = Decoder(hidden_size)\n",
    "\n",
    "        #decode edges:\n",
    "        \n",
    "    \n",
    "    def encode(self, features, edge_index, edge_attr, batch):\n",
    "        return self.encoder(features, edge_index, edge_attr, batch)\n",
    "\n",
    "    def decode(self, nodes_encoded, edges_encoded, batch):\n",
    "        return self.decoder(nodes_encoded, edges_encoded, batch)\n",
    "\n",
    "    def forward(self, features, adj, edge_attr):\n",
    "        nodes_enc, edges_enc = self.encode(features, adj, edge_attr)\n",
    "        nodes_dec, edges_dec = self.decode(nodes_enc, edges_enc)\n",
    "\n",
    "        return nodes_dec, edges_dec\n",
    "    \n",
    "\n",
    "    def compute_loss(self, nodes_generated, nodes_real, edges_generated, edges_real):\n",
    "\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        nodes_loss = loss(nodes_generated, nodes_real)\n",
    "        edges_loss = loss(edges_generated, edge_idx_to_adj(edges_real))\n",
    "        return nodes_loss + edges_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(hidden_size=HIDDEN_SIZE)\n",
    "decoder = Encoder(hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "encoder=encoder.float()\n",
    "decoder = decoder.float()\n",
    "g = GraphNeuralNetwork(hidden_size=512, encoder=encoder, decoder=decoder)\n",
    "#g=g.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import int64\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, l_r = 0.00001, epochs = 100):\n",
    "        # hyperparameters definition\n",
    "        hidden_size = 512\n",
    "        learning_rate = l_r\n",
    "    \n",
    "        #model = GraphNeuralNetwork(hidden_size, -1, 1)  # TODO: you can add more hyperparameters if needed\n",
    "        #model.train()\n",
    "    \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr= learning_rate)\n",
    "        for epoch in trange(1, epochs + 1, leave=False):\n",
    "            model.train(True)\n",
    "            for data in train_loader:\n",
    "                real_features, real_edge_index, real_edge_attr, batch, mol = data.features, data.edge_index.to(dtype=torch.int64), data.edge_attr, data.batch, data.mol\n",
    "                #print(real_edge_index.shape)\n",
    "                x_encoded = model.encode(real_features, real_edge_index, real_edge_attr, batch)\n",
    "                #real_features = F.one_hot(torch.argmax(real_features, dim=1), ATOM_DIM)\n",
    "                x_decoded = model.decode(x_encoded[0], x_encoded[1], batch)\n",
    "                #print(x_decoded[0].shape, x_decoded[1].shape)\n",
    "                #print(x_decoded[0].shape, real_features.shape)\n",
    "                loss = model.compute_loss(x_decoded[0], real_features, x_decoded[1], real_edge_index)\n",
    "                #print(x_decoded[0], real_features, x_decoded[1], real_edge_index)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            # for data in tqdm(valid_loader):\n",
    "            #     x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            #     preds = model(x, edge_index, batch)\n",
    "            #     preds_batches.append(preds.detach().numpy())\n",
    "            #     y_valid_batches.append(y.detach().numpy())\n",
    "            \n",
    "        #return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308172bcb12b4cca9a4b3742d7bd4ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4346, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.7023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9390, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8752, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0996, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1281, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8650, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0370, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2680, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0346, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4982, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2628, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2580, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2626, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6529, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.9005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3914, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1703, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8945, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7648, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3853, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8917, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2556, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7626, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1643, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3795, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8334, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.4008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6221, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0965, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6744, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8765, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2470, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6722, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7468, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7450, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3925, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3602, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3909, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2317, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2409, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2304, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1340, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3524, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2280, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7368, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0839, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1278, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7337, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6637, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3473, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5874, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2248, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8524, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0253, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0795, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3442, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4418, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3428, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5782, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0765, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5760, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6421, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6409, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8442, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6580, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5680, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9997, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6551, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.8001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9982, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3615, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2128, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3277, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6289, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3268, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2116, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6278, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0965, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4289, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6527, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9971, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4277, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9996, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3520, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7943, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6502, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0618, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0865, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2116, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0853, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6491, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3476, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0609, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3465, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3454, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6971, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7914, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0820, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7909, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0809, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4227, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8125, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3423, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4214, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5304, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3372, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5272, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9870, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0702, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.2004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0685, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5214, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.8002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7861, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6442, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6440, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0660, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3278, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5982, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7853, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3091, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6792, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5965, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1999, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7841, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6428, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1997, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0524, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7839, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5128, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7910, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0520, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6423, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6421, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5896, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6715, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0516, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5888, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6419, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0512, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1895, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9745, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6415, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7822, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0540, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0502, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5822, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1861, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6643, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6408, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7820, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5797, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7806, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6619, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.6405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.3089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.3036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.1975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.5789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.7799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.6611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0500, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4167, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(g, train_loader=loader, valid_loader=test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 84], edge_attr=[84, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 78], edge_attr=[78, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 96], edge_attr=[96, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 98], edge_attr=[98, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 92], edge_attr=[92, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 66], edge_attr=[66, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 72], edge_attr=[72, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 80], edge_attr=[80, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 90], edge_attr=[90, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 68], edge_attr=[68, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 88], edge_attr=[88, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 86], edge_attr=[86, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 92], edge_attr=[92, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 88], edge_attr=[88, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n",
      "DataBatch(edge_index=[2, 94], edge_attr=[94, 5], features=[32, 12], mol=[1], batch=[32], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
